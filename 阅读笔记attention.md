# 阅读笔记 

# Attention is all you need

## 1. Abstract

主流的序列转换模型基于包含编码器和解码器的复杂循环或卷积神经网络。表现最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，Transformer，完全基于注意力机制，彻底摒弃了递归和卷积。在两个机器翻译任务的实验中，这些模型在质量上优于其他模型，同时具有更高的并行化能力，并且训练所需的时间大幅减少。我们的模型在 WMT 2014 英德翻译任务中达到了28.4 BLEU，比现有的最佳结果（包括集成模型）提高了超过2 BLEU。在 WMT 2014 英法翻译任务中，我们的模型在使用八个 GPU 训练3.5天后，创下了41.0的新单一模型最高 BLEU 分数，训练成本仅为文献中最佳模型的一小部分。

## 2. pre work'

### 在这里作者提出了一种简单的架构避免了循环神经网络，完全依赖于注意力机制来绘制输出于输入的全局依赖关系

为了减少顺序计算的目标也是扩展神经 GPU [20]、ByteNet [15] 和 ConvS2S [8] 的基础，这些模型都使用**卷积神经网络作为基本构建块**，同时为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联两个任意输入或输出位置的信号所需的操作数量随着位置间的距离而增长，对于 ConvS2S 是线性增长，对于 ByteNet 则是对数增长。这使得学习远距离位置之间的依赖性变得更加困难 [11]。在 Transformer 中，这被减少到常数数量的操作，尽管因为平均注意力加权位置而导致有效分辨率降低，这一效应我们通过在第 3.2 节中描述的多头注意力来对抗。自注意力，有时被称为内部注意力，**是一种关联单个序列的不同位置以计算序列表示的注意力机制**。自注意力已成功用于多种任务，包括阅读理解、抽象性总结、文本蕴含和学习任务独立的句子表示 [4, 22, 23, 19]。**端到端记忆网络基于循环注意力机制**而不是序列对齐的循环神经网络，并已显示出在简单语言问答和语言建模任务 [28] 上表现良好。据我们所知，Transformer 是**第一个完全依赖自注意力来计算其输入和输出表示的转换模型**，而不使用序列对齐的 RNN 或卷积。在接下来的章节中，我们将描述 Transformer，激励自注意力并讨论其相对于诸如 [14, 15] 和其他模型的优势。

## 3. 模型架构

![image-20240418141055916](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418141055916.png)

input sequence $x_1 , x_2 , X_n$ to ($z_1 , ...Z_n$)

decoder change z  t0 $y_1 , y_n$​

每一个时刻 模型都是自回归的 means the previously generated sysmbols as additional input when generating the next

Transformer 遵循这种整体架构，使用编码器和解码器的堆叠自注意力和逐点全连接层，分别如图 1 的左半和右半所示。

### 3.1 encoder ＆ decoder

![image-20240418144311143](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418144311143.png)

![image-20240418144348729](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418144348729.png)

### 3.2Attention

#### 3.2.1 点积放缩注意力机制

![image-20240418144457235](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418144457235.png)

$$Attention(Q,K,V) = softmax\times (\frac{q\times k^T}{\sqrt{d_k}})v$$

Q is 查询 即 query  ; K 是 键 key ; v 是 值 value

使用QK……T之后得到的是一个注意力分数矩阵（张量）

由于QK想成会导致维度的改变  ， 这里的 除以dk相当于(Key_dimension) 调节缩放因子，防止梯度消失

softmax函数彻底把这个分数矩阵转化为一个概率矩阵

last*v means 得到的输出是一个加权和，其中每个Value的权重由它们对应的Key与当前Query的相关性决定



多头指的就是 使用多个注意力机制进行得分分配然后把他们拼接到一起

![image-20240418145335206](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418145335206.png)



在作者的模型中：

1. Encoder-Decoder Attention  ：在这种设置中，查询（Queries）来自解码器的前一层，而记忆键（Memory Keys）和值（Values）则来自编码器的输出。这种注意力层的设计目的是让**解码器能够在生成输出时有效地利用到整个输入序列的信息**，提高翻译或文本生成的相关性和准确性。
2. Self-Attention in the Encoder  在编码器的自注意力层中，键（Keys）、值（Values）和查询（Queries）都来自同一个地方，即编码器当前层的前一层输出。



### 3.3 Position-wise Feed-Forward Networks

![](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418145739964.png)

为啥要折磨做作者在文章里并内有给出具体的说明；；~~我猜想可能是增加模型的非线性 、、 学习能力 、、~~

### 3.4Embeddings and $softmax$

与其他序列转导模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度 dmodel 的向量。我们还使用通常学习的线性变换和 softmax 函数将解码器输出转换为预测的下一个标记概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵，类似于 [24]。在嵌入层中，我们将这些权重乘以 √dmodel。

### 3.5 positional encoding

![image-20240418150551542](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240418150551542.png)

motivation  :  **注入序列信息** 由于Transformer模型中既没有循环机制（如RNN）也没有卷积操作，模型自身无法感知输入序列中令牌的顺序。位置编码通过为每个令牌加入与其位置相关的额外信息，使得模型能够理解词序和语序，这对于语言处理尤为关键。

**保持模型的并行性**：Transformer的一个主要优势是能够并行处理序列数据，而不需要像循环网络那样逐个处理序列中的元素。通过位置编码，可以在保持并行处理能力的同时，让模型利用序列的顺序信息

## Last but not Least

## Why Self-Attention



在本节中，我们将自注意力层与常用于将一个变长符号表示序列 (x1, ..., xn) 映射到另一个等长序列 (z1, ..., zn) 的循环层和卷积层进行了比较，其中 xi, zi ∈ Rd，如典型序列转换编码器或解码器中的隐藏层。为了论证我们使用自注意力的动机，我们考虑了三个要求。一个是每层的总计算复杂性。另一个是可以并行化的计算量，通过所需的最小顺序操作数来衡量。第三个是网络中长距离依赖的路径长度。学习长距离依赖是许多序列转换任务中的一个关键挑战。影响学习这种依赖能力的一个关键因素是信号在网络中必须穿越的路径长度。路径越短，学习长距离依赖就越容易。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。正如表1中所指出的，自注意力层以常数数量的顺序操作连接所有位置，而循环层则需要 O(n) 的顺序操作。在计算复杂性方面，当序列长度 n 小于表示维度 d 时，自注意力层比循环层更快，这在使用由最先进的机器翻译模型使用的句子表示（如词片 [31] 和字节对 [25] 表示）时通常是这种情况。为了提高涉及非常长序列的任务的计算性能，可以将自注意力限制在只考虑围绕各自输出位置的输入序列中大小为 r 的邻域。这将使最大路径长度增加到 O(n/r)。我们计划在未来的工作中进一步研究这种方法。单个卷积层的核宽度 k < n 并不连接所有输入和输出位置的对。这样做需要在连续核的情况下堆叠 O(n/k) 层卷积层，或在扩张卷积的情况下是 O(logk(n))，增加了网络中任意两个位置之间最长路径的长度。卷积层通常比循环层更昂贵，因子为 k。然而，可分离卷积 [6] 大大降低了复杂性，达到 O(k · n · d + n · d^2)。即使 k = n，可分离卷积的复杂性也等同于自注意力层和逐点前馈层的组合，这是我们模型采用的方法。作为一个附带的好处，自注意力可以产生更具可解释性的模型。我们检查了我们模型中的注意力分布，并在附录中展示和讨论了示例。不仅单个注意力头显然学会了执行不同的任务，许多还表现出与句子的句法和语义结构相关的行为。