# 2024.4.16 Self-Attentive Sequential Recommendation

## Q1.摘要



摘要—序贯动态是许多现代推荐系统的关键特征，这些系统试图基于用户最近的行为来捕捉用户活动的“上下文”。为了捕捉这种模式，两种方法广泛传播：**马尔可夫链（MC）**和**循环神经网络（RNN）**。马尔可夫链假设可以基于用户的最后一个（或最后几个）行为来预测用户的下一个行为，而RNN原则上允许揭示更长期的语义。一般来说，在极其稀疏的数据集中，**MC基方法表现最好**，其中模型简约性至关重要；而在数据较密集的数据集中，RNN的表现更佳，因为可以负担更高的模型复杂性。**我们的工作目标是平衡这两个目标，**通过提出一个基于自注意力的序列模型**（SASRec）**，该模型允许我们捕捉长期语义（类似于RNN），但使用注意力机制，**基于相对较少的行为进行预测（类似于MC）**。在每个时间步骤，SASRec试图识别用户行为历史中哪些项目是“相关的”，并使用它们来预测下一个项目。广泛的实证研究表明，**我们的方法在稀疏和密集数据集上都超过了各种最先进的序列模型（包括基于MC/CNN/RNN的方法）**。此外，该模型比可比的CNN/RNN基模型高出一个数量级的效率。注意力权重的可视化也展示了我们的模型如何适应不同密度的数据集，并在活动序列中揭示有意义的模式。

## Q2.Introduction

顺序推荐系统的目标是结合个性化的用户行为模型（基于历史活动）与基于用户最近行为的某种“上下文”概念。从序列动态中捕获有用的模式是具有挑战性的，主要是因为输入空间的维度随着用作上下文的过去行为数量的增加而呈指数级增长。因此，顺序推荐的研究主要关注如何简洁地捕捉**这些高阶动态**。马尔可夫链（MC）是一个经典的例子，它假设下一个行动只受到上一个行动（或前几个行动）的影响，并已成功用于**表征推荐的短范围项目转换**。另一方面，循环神经网络（RNN）通过隐藏状态总结所有之前的行为，该状态用于预测下一个行为。尽管这两种方法在特定情况下表现强劲，但它们在某些类型的数据上有所限制。基于MC的方法通过采用强化简化假设，在高稀疏性设置中表现良好，但可能无法捕捉更复杂场景的复杂动态。相反，虽然RNN具有表现力，但在它们能够超越简单基线之前，需要大量的数据（尤其是密集数据）。

最近，一种名为Transformer的新型序列模型在机器翻译任务中取得了前所未有的性能和效率。与使用卷积或循环模块的现有序列模型不同，Transformer完全基于一种称为“自注意力”的提议注意力机制，这种机制高效能够揭示句子中单词之间的语法和语义模式。

受到这种方法的启发，**我们寻求将自注意力机制应用于序列推荐问题**。我们希望这一思想能够同时解决上述两个问题，一方面能够像RNN那样从过去的所有行为中提取上下文，另一方面能够像MC那样仅根据少数几个行为做出预测。具体来说，我们构建了一个基于自注意力的序列推荐模型**（SASRec）**，该模型在每个时间步骤中对以前的项目适应性地分配权重（见图1）。

提出的模型在几个基准数据集上显著优于现有的基于MC/CNN/RNN的序列推荐方法。特别是，我们检查了数据集稀疏性对模型性能的影响，模型性能与上述模式密切相关。由于自注意力机制，SASRec倾向于在密集数据集上考虑长期依赖性，而在稀疏数据集上关注更近期的活动。这对于适应性地处理不同密度的数据集至关重要。

此外，SASRec的核心组件（即自注意力块）适用于并行加速，使得模型的速度比基于CNN/RNN的替代方案快一个数量级。我们还分析了SASRec的复杂性和可扩展性，进行了全面的消融研究以显示关键组件的效果，并通过可视化注意力权重定性地揭示了模型的行为。

### 综上所述 目前 的序列推荐存在 ： 高阶动态的关系 ；以及稀疏性MC短期RNN之间的平衡关系

这篇论文的创新点：1.自注意力机制的应用	2.结合长短期行为  3.并行加速

## 3.solutuon



在一系列的推荐中，我们给定$s^u = (s_1^u ,...,s_n^u)$​对下一个时刻进行预测

input means $(s_1^u , s_2^u,...,s_{n-1}^u)$

output is $(s_2^u , s_3^u , ... , s_n^u)$

### 3.1 嵌入

![image-20240420164423773](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240420164423773.png)

![image-20240420164800707](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240420164800707.png)

### 3.2 self -attention block

### B. 自注意力块（Self-Attention Block）

自注意力机制使用的是缩放点积注意力，它定义为：

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V $$



其中，\( Q \) 表示查询（queries），\( K \) 表示键（keys），\( V \) 表示值（values），每一行代表一个项目。直观上讲，注意力层计算所有值的加权和，其中权重 \( i \) 和 \( j \) 之间的关系取决于查询 \( i \) 和键 \( j \) 之间的相互作用。这里的比例因子 \( \sqrt{d} \) 用来避免内积在维度很高时变得过大。

自注意力层：在像机器翻译这样的自然语言处理任务中，注意力机制通常与 \( K = V \) 一起使用（例如，在使用RNN编解码器进行翻译时：编码器的隐藏状态作为键和值，解码器的隐藏状态作为查询）【28】。最近，提出了一种自注意力方法，它使用相同的对象作为查询、键和值【3】。在我们的案例中，自注意力操作接受嵌入 \( \hat{E} \) 作为输入，通过线性投影将其转换为三个矩阵，然后将它们送入一个注意力层：

$$ S = \text{SA}(\hat{E}) = \text{Attention}(\hat{E}W_Q, \hat{E}W_K, \hat{E}W_V) $$



其中投影矩阵 $$( W_Q, W_K, W_V \in \mathbb{R}^{d \times d} )$$。这些投影使模型更加灵活。例如，模型可以学习非对称的相互作用（即 \( <\text{query } i, \text{ key } j> \) 和 \( <\text{query } j, \text{ key } i> \) 可以具有不同的相互作用）。

因果性：由于序列的本质，模型在预测第 \( t+1 \) 个项目时只应考虑前 \( t \) 个项目。然而，自注意力层的第 \( t \) 个输出 \( S_t \) 包含了后续项目的嵌入，这使得模型设定不妥。因此，我们通过禁止所有 \( Q_i \) 和 \( K_j \) （\( j > i \)）之间的链接来修改注意力。

逐点前馈网络：尽管自注意力能够以自适应的权重聚合所有先前项目的嵌入，但它本质上仍是一个线性模型。为了赋予模型非线性，并考虑不同潜在维度之间的相互作用，我们对所有 \( S_i \) 应用逐点的两层前馈网络（共享参数）：

$[ F_i = \text{FFN}(S_i) = \text{ReLU}(S_iW^{(1)} + b^{(1)})W^{(2)} + b^{(2)}]$​



### 2.3 堆叠自注意力



![image-20240420165603857](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240420165603857.png)

![image-20240420165801334](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240420165801334.png)

### 3.4 预测能

![image-20240420170012806](C:\Users\杜冠辰\AppData\Roaming\Typora\typora-user-images\image-20240420170012806.png)
